---
layout: post
title : K8s
author: MÃ©gane Vilain
category: Kubernetes
---

# Kubernetes
Kubernetes is an open-source container orchestration system for automating software deployment, scaling, and management. Google originally designed Kubernetes, but the Cloud Native Computing Foundation now maintains the project.

## Concepts 

![Architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/b/be/Kubernetes.png/330px-Kubernetes.png)

## Control plane
The Kubernetes master node handles the Kubernetes control plane of the cluster, managing its workload and directing communication across the system. The Kubernetes control plane consists of various components, each its own process, that can run both on a single master node or on multiple masters supporting high-availability clusters.
The various components of the Kubernetes control plane are as follows:

### Etcd
Persistent, lightweight, distributed, key-value data store. It stores the configuration data of the cluster. 
### Api server
Serves the kubernetes api using JSON over HTTP, which provides both the internal and external interface to Kubernetes. The API server processes and validates REST request and update the state of the API objects in etcd, thereby allowing clients to configure workloads and containers accros workers nodes. The api server uses etcd's watch API to monitor the cluster, roll out critical configuration changes. As an example, the deployer may specify that three instances of a particular "pod" need to be running. etcd stores this fact. If the Deployment Controller finds that only two instances are running (conflicting with the etcd declaration), it schedules the creation of an additional instance of that pod.

### Scheduler
Extensible component that selects on which node an unschedules pod runds, based on ressource availability. The scheduler tracks ressouce use on each node to ensure that workload is not scheduled in excess of available resources. His role is to match resource "supply" to workload "demand"

### Controller
Reconciliation loop that drives the actual cluster state toward the desired state, communicating with the API server to create, update and delete the resources it manages. One kind of controller is a Replication controller, which handes replication and scaling by running a specified number of copies of a pod across the cluster. It also handles creating replacement pods if the underlying node fails. Other controllers that are part of the core Kubernets system include a DaemonSett Controller for running exactly one pod on every machine, and a Job Controller for running pods that run to completion, as part of a batch job.

### Controller manager
Process that manages a set of core Kubernetes controllers.

## Namespaces
Kubernetes provides a partitioning of the ressources it manages into non-overlapping sets called namespaces. They are intented for use in environments with many users spread across multiple teams, or projects, or enven separating environments like development, test and production.

## Nodes

A node, also known as a worker or a minion, is a machine where containers (workloads are deployed. Every node in the cluster must run a container runtime (containerd) for communication with primary for network configuration of these containers.

### Kubelet

Responsible for the running state of each node, ensuring that all containers on the node are healthy. It takes care of starting, stopping, and mainting application containers organized into pods. Kubelet monitors the state of a pod, and if not in the desired state, the pod re-deploys the same node. Node status is relayed every few seconds via heartbeat messages to the primary. Once the primary detects a node failure, the Replication Controller observes this state change and launches pods on other healthy nodes.

### Kube-proxy
Implementation of a network proxy and a load balancer. Responsible for routing traffic to the appropriate container based on IP and port number of the incoming requqest.

### Container
Resides inside a pod. Lowest level of a microservice, which holds the running applicatioon, librairies and their dependencies. Can be exposed to the world through an external IP adress. 

## Pods

The basic sheduling unit in K8s is a pod, which consists of one or more containers that are guaranteed to be co-located on the same node. Each pod in K8s is assigned a unique IP address within the cluster.
A pod can define a volume, such as local disk directory or a network disk and expose it to the containers in the pod. Pods can be managed manually through the K8s API, or their management can be delegated to a controller.

### DaemonSets
Normally, the K8s Scheduler decides where to run pods. For some use cases, though, there could be a need to run a pod on every single node in the cluster. This is useful for use case like log, ingress controller and storage services.

### Replicat Sets
It's purpose is to maintain a stable set of replicat pods running at any given time.
The definition of a replica set uses a selector, whose evaluation will result in indentifying all pods that are associated with it.

### Service
Set of pods that work together. The set of pods that constitute a service are defined by a label selector. By default a service is exposed inside a cluster but a service can also be exposed outside a cluter (for clients to reach front-end pods)

### Volumes

File systems in K8s container provide ephemeral storage, by default. This means that a restart of the pod will wipe out any data on such containers.
A K8s volume provides persitent storage that exists for the lifetime of the pod itseld. This storage can also be used as shared disk space for containers within the pods. Volumes are mounted at specific mounts points within the container, which are defined by the pod configuration, and connot mount onto other volumes or link to other volumes. The same volume can be mounted at different points in the file system by different containers.

### ConfigMaps and Secret
K8s provides tow closely related mechanisms to deal with this need: Configsmaps and secrets, both of which allow for configuration changes to be made without requiring an application build. The data will be made available to every single instance of the application to which these objects have been bound via the deployment. A secret or configmap is sent to a node only if a pod on that node requires it. K8s will it in memory on that node. Once the pod that depends on the secret or configmap is deleted, the in memory copy are deleted as well.
The data is accessible to the pod through one of the two ways:

- Environment variables
- Available on the container file system that is visible only from within the pod

The data itself is stored on the master which is a highly secured machine which nobody shoud have login access to. 
The biggest difference between a secret and a configmap is that the content of the data in a secret is base 64 encoded.

### StatefulSets
Scaling stateless applications is only a matter of adding more running pods. Stateful workloads are harder, because the state needs to be preserved if a pod is restarted. If the application is scaled up or down, the state may need to be redistributed. Databases are an example of stateful workloads.
StatefulSets are controllers that enforce the properties of uniqueness and ordering amongst instances of a pod and can be used to run stateful applications.

### Deployments
Deployments will manage what happens to the replicaSet - whether an update has to be rolled out, or rolled back. When deployments are scaled up or down, this results in the declaration of the replica set changing and this change is managed by the replication controller.

### Labels and selectors
Kubernetes enables clients (users or internal components) to attach keys called "labels" to any API object in the system, such as pods and nodes. Correspondingly, "label selectors" are queries against labels that resolve to matching objects.
When a service is defined, one can define the label selectors that will be used by the service router/load balancer to select the pod instances that the traffic will be routed to.
